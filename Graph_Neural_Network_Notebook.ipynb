{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credits\n",
        "\n",
        "The code in this notebook was based on the tutorials provided by [Maxim Labonne](https://mlabonne.github.io/blog/about.html) and Ms. Jazzie Jao.\n",
        "\n",
        "Specifically these set of tutorials:\n",
        "- [Graph Convolutional Networks](https://mlabonne.github.io/blog/posts/2022_02_20_Graph_Convolution_Network.html)\n",
        "- [Graph Attention Networks](https://mlabonne.github.io/blog/posts/2022-03-09-Graph_Attention_Network.html)\n",
        "- [GraphSAGE](https://mlabonne.github.io/blog/posts/2022-04-06-GraphSAGE.html)\n",
        "- Tutorial provided by Ms. Jao"
      ],
      "metadata": {
        "id": "ZcX3Q0NtSL79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Used"
      ],
      "metadata": {
        "id": "bmt7wRQ5UYBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** this was coded in Google Colab. It is recommended that you run this in Colab."
      ],
      "metadata": {
        "id": "OMahN7947ZfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "nYCcPy7uKCVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg-lib -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "id": "loyyW_eWKt3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu124.html\n",
        "!pip install torch-sparse\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "collapsed": true,
        "id": "37oPQsLdUXtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.utils import to_dense_adj, to_networkx\n",
        "\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.loader import NeighborLoader"
      ],
      "metadata": {
        "id": "5YyAxMunUe9q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "EjzWvxfBR_vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset that we will be using is the Pubmed dataset.\n",
        "\n",
        "The dataset consists of 19,717 nodes. These nodes represent a publication about diabetes.\n",
        "\n",
        "Each node has 500 features where each represent a weighted word vector [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
      ],
      "metadata": {
        "id": "jLTES8nsNqYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset from PyTorch Geometric\n",
        "dataset = Planetoid(root=\".\", name=\"Pubmed\")\n",
        "data = dataset[0]\n",
        "\n",
        "print(f'Number of nodes: {data.x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYkg1iZOVMY",
        "outputId": "f6bbcca2-f0da-41b5-82f7-ec5fb44e631b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 19717\n",
            "Number of features: 500\n",
            "Number of classes: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our Problem\n",
        "\n",
        "Our problem now is to identify what diabetes type is the topic in each paper. There are 3 types of diabetes, thus 3 classes.\n",
        "\n",
        "We can do that by training a model to identify what type is that particular paper (represented as a node). This is essentially node classification."
      ],
      "metadata": {
        "id": "0dSTCOO6OwsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Representation\n",
        "\n",
        "Let us look into how the given dataset graph is represented"
      ],
      "metadata": {
        "id": "pAtkMZNPQJaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4U_uJSQS--",
        "outputId": "a80df4ce-719b-47a4-b72d-891bd4e8c990"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collection of nodes with Graph Neural Networks are often represented as a node feature matrix, where each row represents a node while each column represents a feature. This is why the `x` variable in `data` has 19717 rows and 500 columns.\n",
        "\n",
        "`y` represents the ground truth or the label of each node.\n",
        "\n",
        "`train_mask`, `val_mask`, and `test_mask` is an array or `true` and false` this is used to identify which nodes are used for training, validation, and testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "4iybi-5XQiXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge representation\n",
        "\n",
        "The edges `edge_index` are represented as a coordinate list.\n",
        "\n",
        "It is a 2 by `x` where `x` represents the number of edges the graph has.\n",
        "\n",
        "The first row represents the starting connection. The second row represents the end connection.\n",
        "\n",
        "Each column represents an edge connection.\n",
        "\n",
        "### Example\n",
        "We have a small given `edge_index` of 2x3, meaning we have 3 number of edges\n",
        "```\n",
        "[[1, 2, 3],\n",
        "[1, 1, 2]]\n",
        "```\n",
        "From this, we can see that node 1 is connected with node 1, node 2 is connected with node 1, node 3 is connected with node 2."
      ],
      "metadata": {
        "id": "rlp8AWC_RTz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the `edge_index` of our graph"
      ],
      "metadata": {
        "id": "UEUykA9PFgnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'edge_index = {data.edge_index.shape}')\n",
        "print(data.edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3v9UNxYFD_f",
        "outputId": "123db7bc-0bf2-4d15-806c-135b537d3c5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge_index = torch.Size([2, 88648])\n",
            "tensor([[ 1378,  1544,  6092,  ..., 12278,  4284, 16030],\n",
            "        [    0,     0,     0,  ..., 19714, 19715, 19716]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operations of Graph Neural Networks\n",
        "\n",
        "1. Aggregate information from neighbor nodes\n",
        "2. With the transformed nodes, apply a learnable function (such as Multilayer perceptrons) to each node.\n",
        "\n"
      ],
      "metadata": {
        "id": "JPj_bYunIe0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type of Graph Neural Networks\n",
        "For now, we have 3 popular types of Graph Neural Networks (GNN)\n",
        "1. Graph Convolutional Network\n",
        "2. Graph Attention Network\n",
        "3. GraphSAGE\n",
        "\n",
        "Each type of GNN has their pros and cons.\n",
        "\n",
        "In this notebook, we will see the difference between these GNNs."
      ],
      "metadata": {
        "id": "Flm2ZsNKt8ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Convolutional Network"
      ],
      "metadata": {
        "id": "gyIEsr6jSC-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section provides a comprehensive guide to understanding and implementing Graph Convolutional Networks (GCNs) using the PyTorch Geometric library."
      ],
      "metadata": {
        "id": "E9Y7QpRNSPhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the simplest architecture out of all the GNNs avaialble. Essentially what we do is add all the vectors of the neighboring nodes.\n",
        "\n",
        "The Graph Convolution operation aggregates information from a node's neighbors:\n",
        "\n",
        "$h_i = \\sum_{j \\in \\tilde{\\mathcal{N}}_i} \\dfrac{1}{\\sqrt{deg(i)} \\sqrt{deg(j)}} W x_j$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\tilde{\\mathcal{N}}$ represents the set of neighbors of node `i`\n",
        "- $deg(i)$ represent the number of neighbors node `i` has\n",
        "- $deg(j)$ represents the number of neighbor's neighbor.\n",
        "- $h_i$ represents the output for node `i`\n",
        "- $W$ is a learnable weight matrix\n",
        "\n",
        "This formula normalizes neighbor information and applies a shared weight matrix.\n",
        "\n",
        "- $\\frac{1}{deg(i)}$ normalizes a node with a lot of neighbors. Since we are adding all vectors of all neighbors, the more neighbors a node has, the higher the value. Without this normalization, some nodes would have a higher value, making an imbalanced data set.\n",
        "- $\\frac{1}{deg(j)}$ normalizes a neighbor. From a [study](https://arxiv.org/abs/1609.02907) by Kipf et al. (2016), a node with a lot of neighbors propagates much more easily. Without control, this can lead to imbalanced data."
      ],
      "metadata": {
        "id": "94QrllpkGE4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "In python, the GNN models can be represented as a class.\n",
        "\n",
        "Each class can have 3 functions:\n",
        "- Initialization\n",
        "- Feed forward\n",
        "- Fitting"
      ],
      "metadata": {
        "id": "ZbhAPrVLK8Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "The main focus of initialization is to setup what type of layers that you may have and the optimizer algorithm of your choosing.\n",
        "\n",
        "```python\n",
        "def __init__(self, dim_in, dim_h, dim_out):\n",
        "  super().__init__()\n",
        "  self.gcn1 = GCNConv(dim_in, dim_h)\n",
        "  self.gcn2 = GCNConv(dim_h, dim_out)\n",
        "  self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                    lr=0.01,\n",
        "                                    weight_decay=5e-4)\n",
        "```\n",
        "\n",
        "Where `dim_in` represent the dimension of the input, `dim_h` represent the number of nodes in each hidden layer, `dim_out` represent the dimension of the output layer which in this case is 7 since we are trying to classify a node which of the 7 paper type it is.\n",
        "\n",
        "For this notebook, we will be using the Adam optimizer. This optimizer is out of scope of this lecture. But essentially, this optimizer adjusts the learning weight for each iteration for faster learning while avoiding overshooting."
      ],
      "metadata": {
        "id": "dMBojqYFNSJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward\n",
        "Feed forward would be the whole structure on how the node embedding is inputted and outputted.\n",
        "\n",
        "```python\n",
        "def forward(self, x, edge_index):\n",
        "  h = F.dropout(x, p=0.5, training=self.training)\n",
        "  h = self.gcn1(h, edge_index).relu()\n",
        "  h = F.dropout(h, p=0.5, training=self.training)\n",
        "  h = self.gcn2(h, edge_index)\n",
        "  return F.log_softmax(h, dim=1)\n",
        "```\n",
        "\n",
        "For this implementation, we will be using 2 layers and 1 output layer. The output is then fed into a softmax function.\n",
        "\n",
        "We chose to do a dropout (not pass any data) 50% so that the model can be more generalizable to other unseen data. This is only done during training (when training flag is set to true)\n",
        "\n",
        "Visually, we have this pipeline\n",
        "![](https://drive.usercontent.google.com/download?id=1wGLl4BXO85rZqX9snpnR1NYK32DdwxLe&export=view&authuser=1)\n",
        "\n",
        "Where ***W*** represents the weight being used in all connections under it."
      ],
      "metadata": {
        "id": "3X3a8-3KOEze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting (optimizing weights)\n",
        "To optimize the weights W in our graph convolutional network, we employ the Adam optimizer. Since this is a multiclass problem, we will use Cross Entropy Loss as our loss function.\n",
        "\n",
        "```python\n",
        "def fit(self, data, epochs):\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = self.optimizer\n",
        "\n",
        "  self.train()\n",
        "  for epoch in range(epochs+1):\n",
        "      # Training\n",
        "      optimizer.zero_grad()\n",
        "      out = self(data.x, data.edge_index)\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "      acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
        "                      data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Validation\n",
        "      val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "      val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
        "                          data.y[data.val_mask])\n",
        "\n",
        "      # Print metrics every 10 epochs\n",
        "      if(epoch % 10 == 0):\n",
        "          print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
        "                f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
        "                f'Val Acc: {val_acc*100:.2f}%')\n",
        "```"
      ],
      "metadata": {
        "id": "69L2GR2M7OE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`loss.backward()` computes the gradients based on the loss function and `optimizer.step()` uses these gradients to optimize the weights of the model.\n",
        "\n",
        "For every epoch, the loss and accuracy is recorded. The validation and training set is being used for this.\n",
        "\n",
        "For better understanding, we will print the results of the model every 10 epochs."
      ],
      "metadata": {
        "id": "z_FRuveD-Pre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final model"
      ],
      "metadata": {
        "id": "ALHD3JfZpj7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We incorporate the final model with the code below"
      ],
      "metadata": {
        "id": "ICt6bjiBpmhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "  \"\"\"Graph Convolutional Network\"\"\"\n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
        "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                      lr=0.01,\n",
        "                                      weight_decay=5e-4)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = F.dropout(x, p=0.5, training=self.training)\n",
        "    h = self.gcn1(h, edge_index).relu()\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.gcn2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1)\n",
        "\n",
        "  def fit(self, data, epochs):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = self.optimizer\n",
        "\n",
        "    self.train()\n",
        "    for epoch in range(epochs+1):\n",
        "        # Training\n",
        "        optimizer.zero_grad()\n",
        "        out = self(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
        "                       data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "        val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
        "                           data.y[data.val_mask])\n",
        "\n",
        "        # Print metrics every 10 epochs\n",
        "        if(epoch % 10 == 0):\n",
        "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
        "                  f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
        "                  f'Val Acc: {val_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "AmnfeN62pl08"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing of model"
      ],
      "metadata": {
        "id": "XZOXTmZ9qohH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics for testing\n",
        "\n",
        "We will be measurin 2 metrics for our model\n",
        "- Accuracy\n",
        "- Training Time\n",
        "\n",
        "Accuracy tells us how good our model is with solving the problem. Training time tells us how fast can we train the model.\n",
        "\n",
        "Training time is important since we want to know if a model is *scalable*. If a model's training time raises fast, then it would take a lot of time for it to train on new data."
      ],
      "metadata": {
        "id": "7uETBLFdqyU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test our model for acccuracy the function below for meassuring accuracy"
      ],
      "metadata": {
        "id": "lPdp8UzUqqs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n",
        "    model.eval() # sets the model into eval mode\n",
        "    out = model(data.x, data.edge_index)\n",
        "    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
        "    return acc\n",
        "\n",
        "# We will use %%time to measure execution time."
      ],
      "metadata": {
        "id": "Ku27BFTaqukC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing results"
      ],
      "metadata": {
        "id": "odgU8rfws2HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now see how the model improves through training and the training time.\n",
        "\n",
        "We will use 200 epochs to train the model. You can choose to have lower epochs but for the sake of measuring the time, we will use 200 epochs for all models"
      ],
      "metadata": {
        "id": "VBK40Yn3s5yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create GCN\n",
        "gcn = GCN(dataset.num_features, 64, dataset.num_classes)\n",
        "print(gcn)\n",
        "\n",
        "# Train\n",
        "gcn.fit(data, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnC3efQ0s92W",
        "outputId": "cc834966-6206-4491-9649-4d8160fcb8b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (gcn1): GCNConv(500, 64)\n",
            "  (gcn2): GCNConv(64, 3)\n",
            ")\n",
            "Epoch   0 | Train Loss: 1.098 | Train Acc:  35.00% | Val Loss: 1.10 | Val Acc: 40.20%\n",
            "Epoch  10 | Train Loss: 0.749 | Train Acc:  91.67% | Val Loss: 0.88 | Val Acc: 71.60%\n",
            "Epoch  20 | Train Loss: 0.401 | Train Acc:  93.33% | Val Loss: 0.70 | Val Acc: 75.60%\n",
            "Epoch  30 | Train Loss: 0.197 | Train Acc:  98.33% | Val Loss: 0.64 | Val Acc: 73.60%\n",
            "Epoch  40 | Train Loss: 0.134 | Train Acc:  98.33% | Val Loss: 0.62 | Val Acc: 78.00%\n",
            "Epoch  50 | Train Loss: 0.102 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 76.60%\n",
            "Epoch  60 | Train Loss: 0.106 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 76.40%\n",
            "Epoch  70 | Train Loss: 0.101 | Train Acc:  98.33% | Val Loss: 0.62 | Val Acc: 74.60%\n",
            "Epoch  80 | Train Loss: 0.078 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.00%\n",
            "Epoch  90 | Train Loss: 0.068 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.20%\n",
            "Epoch 100 | Train Loss: 0.089 | Train Acc:  98.33% | Val Loss: 0.61 | Val Acc: 77.00%\n",
            "Epoch 110 | Train Loss: 0.065 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 77.20%\n",
            "Epoch 120 | Train Loss: 0.064 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 77.00%\n",
            "Epoch 130 | Train Loss: 0.069 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.80%\n",
            "Epoch 140 | Train Loss: 0.070 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 78.60%\n",
            "Epoch 150 | Train Loss: 0.067 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.80%\n",
            "Epoch 160 | Train Loss: 0.072 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 76.60%\n",
            "Epoch 170 | Train Loss: 0.090 | Train Acc:  98.33% | Val Loss: 0.64 | Val Acc: 76.40%\n",
            "Epoch 180 | Train Loss: 0.071 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 76.80%\n",
            "Epoch 190 | Train Loss: 0.058 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.80%\n",
            "Epoch 200 | Train Loss: 0.051 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 75.40%\n",
            "CPU times: user 32.9 s, sys: 9.31 s, total: 42.2 s\n",
            "Wall time: 47.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results steadies at around 30 epochs. The training time is 47.3 seconds."
      ],
      "metadata": {
        "id": "T1CBkh2stVOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nGCN test accuracy: {test(gcn, data)*100:.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFpfvDEhufjm",
        "outputId": "a778b3b9-ddd6-47b2-c088-56aaf6104970"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GCN test accuracy: 79.10%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the Pubmed dataset, Graph Convolutional Network scored 79.10% for test accuracy."
      ],
      "metadata": {
        "id": "jhQVZyeyu6YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph Attention Network"
      ],
      "metadata": {
        "id": "doYMSVa9SH4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Attention Networks (GAT) works similarly with GCN. The decerning factor is that when it is time for aggregating, the neighbors of a node don't have equal importance.\n",
        "\n",
        "Some neigbors of a node may be more relevant than other neighbors. We call this relevance as *attention*. Attention is a weight that is multiplied to a tensor to scale its value.\n",
        "\n",
        "How do we then compute the attention?\n",
        "\n",
        "*Linear Transformation*\n",
        "\n",
        "$a_{ij} = W^t_{att}[Wx_i || Wx_j]$\n",
        "\n",
        "For each node, we have a learnable attention weight $W^t_{att}$. This is multiplied with the concatention of the dot product between a fixed weight matrix $W$ and the node i vector and neighbor node j vector.\n",
        "\n",
        "*Pass it through an activation function*\n",
        "\n",
        "To add non linearlity, we will pass the linear transformation into a nonlinear function\n",
        "\n",
        "$e_{ij} = LeakyReLU(a_{ij})$\n",
        "\n",
        "*Repeat the computation of $e$ for all neighbors*\n",
        "\n",
        "${\\forall}k(e_{ik} = LeakyReLU(a_{ij})| k \\in \\tilde{\\mathcal{N}}_i)$\n",
        "\n",
        "We then pass it through a softmax function to get the attention distribution of the neighbors for node i\n",
        "\n",
        "Attention between node i and j:\n",
        "\n",
        "$\\alpha_{ij} = softmax(e_{ij})$\n",
        "\n",
        "There can also be multiple attentions computed (multiple attention heads). This is so that the model can more robust and can be more stable. This multiple attention is calcualted with different attention weights $W^t_{att}$. All the computed attentions are multiplied to the respective neighbors creating a *variant* for each neighbor. All variants of a neighbor are concatenated together and averged on the output layer.\n"
      ],
      "metadata": {
        "id": "L_Yv_R18R-gS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "OiRsLPMn5KDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has a similar implementation to GCNs. The only difference is the model that we use.\n",
        "\n",
        "For our implementation, we would be using 8 attention heads.\n",
        "\n",
        "```python\n",
        "def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
        "    super().__init__()\n",
        "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
        "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                      lr=0.005,\n",
        "                                      weight_decay=5e-4)\n",
        "```\n",
        "Instead of using the `GCNConv` class, we use the `GATv2Conv` class."
      ],
      "metadata": {
        "id": "nCnAqMaPNEai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the earlier's illustration, we now have an additional weight $\\alpha$ that tells us how important that neightbor is.\n",
        "\n",
        "![](https://drive.usercontent.google.com/download?id=1t4XZKJH7R_bBfaJ9aPx9-_PF8zZx3R-0&export=view&authuser=1)"
      ],
      "metadata": {
        "id": "HNOtB-7BkSbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where ***W*** represents all the weights under it while **$a$** represents the attention coefficent being multiplied to the respective neighbors of node i."
      ],
      "metadata": {
        "id": "c8viH4dH7B34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(torch.nn.Module):\n",
        "  \"\"\"Graph Attention Network\"\"\"\n",
        "  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
        "    super().__init__()\n",
        "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
        "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                      lr=0.005,\n",
        "                                      weight_decay=5e-4)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = F.dropout(x, p=0.7, training=self.training)\n",
        "    h = self.gat1(x, edge_index).relu()\n",
        "    h = F.dropout(h, p=0.7, training=self.training)\n",
        "    h = self.gat2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1)\n",
        "\n",
        "  def fit(self, data, epochs):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = self.optimizer\n",
        "\n",
        "    self.train()\n",
        "    for epoch in range(epochs+1):\n",
        "        # Training\n",
        "        optimizer.zero_grad()\n",
        "        out = self(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
        "                       data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "        val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
        "                           data.y[data.val_mask])\n",
        "\n",
        "        # Print metrics every 10 epochs\n",
        "        if(epoch % 10 == 0):\n",
        "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
        "                  f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
        "                  f'Val Acc: {val_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "eFoe6hit5Jz6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing of model"
      ],
      "metadata": {
        "id": "ggGEw_Udkz8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create GCN\n",
        "gat = GAT(dataset.num_features, 64, dataset.num_classes)\n",
        "print(GAT)\n",
        "\n",
        "# Train\n",
        "gat.fit(data, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM7eOtn1k8xK",
        "outputId": "cf42f715-f1c2-4d0c-8f2d-586303914217"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.GAT'>\n",
            "Epoch   0 | Train Loss: 3.165 | Train Acc:   1.67% | Val Loss: 3.16 | Val Acc: 1.80%\n",
            "Epoch  10 | Train Loss: 0.934 | Train Acc:  75.00% | Val Loss: 1.02 | Val Acc: 64.80%\n",
            "Epoch  20 | Train Loss: 0.562 | Train Acc:  88.33% | Val Loss: 0.77 | Val Acc: 73.60%\n",
            "Epoch  30 | Train Loss: 0.302 | Train Acc:  93.33% | Val Loss: 0.68 | Val Acc: 73.60%\n",
            "Epoch  40 | Train Loss: 0.187 | Train Acc:  95.00% | Val Loss: 0.62 | Val Acc: 74.40%\n",
            "Epoch  50 | Train Loss: 0.111 | Train Acc:  98.33% | Val Loss: 0.58 | Val Acc: 76.60%\n",
            "Epoch  60 | Train Loss: 0.091 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 75.80%\n",
            "Epoch  70 | Train Loss: 0.083 | Train Acc:  98.33% | Val Loss: 0.62 | Val Acc: 76.20%\n",
            "Epoch  80 | Train Loss: 0.064 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 77.00%\n",
            "Epoch  90 | Train Loss: 0.067 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 74.80%\n",
            "Epoch 100 | Train Loss: 0.050 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.40%\n",
            "Epoch 110 | Train Loss: 0.051 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.80%\n",
            "Epoch 120 | Train Loss: 0.051 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 75.80%\n",
            "Epoch 130 | Train Loss: 0.048 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.00%\n",
            "Epoch 140 | Train Loss: 0.042 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 77.00%\n",
            "Epoch 150 | Train Loss: 0.042 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 77.00%\n",
            "Epoch 160 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 0.66 | Val Acc: 76.00%\n",
            "Epoch 170 | Train Loss: 0.035 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.20%\n",
            "Epoch 180 | Train Loss: 0.035 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 76.40%\n",
            "Epoch 190 | Train Loss: 0.032 | Train Acc: 100.00% | Val Loss: 0.68 | Val Acc: 76.40%\n",
            "Epoch 200 | Train Loss: 0.029 | Train Acc: 100.00% | Val Loss: 0.70 | Val Acc: 74.60%\n",
            "CPU times: user 5min 42s, sys: 6min 34s, total: 12min 17s\n",
            "Wall time: 12min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it took significantly more time to train the graph attention network. This is because aside from optimizing the weights of the network, we also optimize the weights for the computation of all *attention* in the graph."
      ],
      "metadata": {
        "id": "XHHloMOHrBRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nGAT test accuracy: {test(gat, data)*100:.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgaf7lEipiua",
        "outputId": "d982a5ed-9844-400a-a2d5-85c27495131a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GAT test accuracy: 77.90%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the accuracy of both GCN and GAT is not far from each other. But, the training time of GAT is significantly higher.\n",
        "\n",
        "This does not mean that GATs are inferior. It just means that this would not be the right dataset to use a GAT. Since GAT stores additional information through attention, we can say that this is a much more powerful model. We can see the benefits of using GAT with much more complex datasets."
      ],
      "metadata": {
        "id": "iJ0q7fsnB3uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE"
      ],
      "metadata": {
        "id": "zNH4b5yYSJ4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GraphSAGE employs a similar aggregation strategy with all the GNN architectures. What it does differntly is sampling.\n",
        "\n",
        "With GCN and GATs we aggregate all the node's neighbor. This can be a problem since when the graph has a lot of nodes, the computational cost rapidly rises.\n",
        "\n",
        "The first objective of GraphSAGE is to reduce the number of nodes through sampling without too much compromise of the accuracy of the model.\n",
        "\n",
        "This is the algorithm for sampling\n",
        "```\n",
        "For each node:\n",
        "  Choose j number of neighbors randomly\n",
        "    For every neighbor\n",
        "      choose k number of neighbors randomly\n",
        "        ...\n",
        "    Drop unchosen neighbors\n",
        "  Drop unchosen neighbors\n",
        "```\n",
        "We can choose to have as many hop neighbors as we want, but for our implementation we would use `[5, 10]`. Meaning we would choose 5 neighbors and 10 neigbors for each of those 5 neighbors. The centeral node is called seed node.\n",
        "\n",
        "For better efficiency, we can also split the whole graph into subgraphs. We can do this by creating batches. This is where batch size matters. Batch size tells us how many number of seed nodes would be used per batch. For example if we have 20 nodes and a batch size of 10, we would have 2 subgraphs. Each subgrpah contains 10 seed nodes and their corresponding sampled neighbors."
      ],
      "metadata": {
        "id": "BkuA7AKnFBrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create batches with neighbor sampling\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[5, 10],\n",
        "    batch_size=16,\n",
        "    input_nodes=data.train_mask,\n",
        ")\n",
        "\n",
        "# Print each subgraph\n",
        "for i, subgraph in enumerate(train_loader):\n",
        "    print(f'Subgraph {i}: {subgraph}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1w375onCDhH",
        "outputId": "5f9c62b4-e9f3-4ba3-bc4b-85967f82230b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subgraph 0: Data(x=[370, 500], edge_index=[2, 426], y=[370], train_mask=[370], val_mask=[370], test_mask=[370], n_id=[370], e_id=[426], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[16], batch_size=16)\n",
            "Subgraph 1: Data(x=[249, 500], edge_index=[2, 299], y=[249], train_mask=[249], val_mask=[249], test_mask=[249], n_id=[249], e_id=[299], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[16], batch_size=16)\n",
            "Subgraph 2: Data(x=[282, 500], edge_index=[2, 322], y=[282], train_mask=[282], val_mask=[282], test_mask=[282], n_id=[282], e_id=[322], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[16], batch_size=16)\n",
            "Subgraph 3: Data(x=[192, 500], edge_index=[2, 227], y=[192], train_mask=[192], val_mask=[192], test_mask=[192], n_id=[192], e_id=[227], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[12], batch_size=12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that subgraphs are avaialble, all seed nodes in the subgraph will be aggregated with their neighbors.\n",
        "\n",
        "For GraphSAGE, there are currenlty 3 ways to aggregate.\n",
        "1. Mean aggregator: All neighbor nodes are averaged and then multiplied to a weight.\n",
        "2. LSTM: Each neighbor is fed through an LSTM to produce a final vector. Since there is not sequence between the neighbors, the neighbros are fed randomly through the LSTM.\n",
        "3. Pooling: All neighbors are passed through a neural network and then the max element out of all the vectors is recorded.\n",
        "\n",
        "Out of all the aggregators, LSTM is the most accurate one. But the mean is the most easy to implement. For this notebook, we will only use the mean aggreagator which has this formula\n",
        "\n",
        "$h_i = W \\cdot mean_{j\\in\\tilde{\\mathcal{N}}}(h_{ij})$"
      ],
      "metadata": {
        "id": "6UlxqNJmf0Ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "NZLEH_pLYLzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to setup the batches in the model's initialization\n",
        "\n",
        "```python\n",
        "def __init__(self, dim_in, dim_h, dim_out, data):\n",
        "  super().__init__()\n",
        "  self.sage1 = SAGEConv(dim_in, dim_h)\n",
        "  self.sage2 = SAGEConv(dim_h, dim_out)\n",
        "  self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                    lr=0.01,\n",
        "                                    weight_decay=5e-4)\n",
        "  self.train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[5, 10],\n",
        "    batch_size=16,\n",
        "    input_nodes=data.train_mask,\n",
        "  )\n",
        "```"
      ],
      "metadata": {
        "id": "hGYyxHD0Yr4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward method is the same structure used with GCNs and GAT"
      ],
      "metadata": {
        "id": "JZQYuEweZC3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the fitting, since we have batches (subgraphs), we iterate through all of them. For each subgraph, we optimize the weights of the model\n",
        "\n",
        "```python\n",
        "def fit(self, data, epochs):\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = self.optimizer\n",
        "\n",
        "  self.train()\n",
        "  for epoch in range(epochs+1):\n",
        "    total_loss = 0\n",
        "    acc = 0\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    # Train on batches\n",
        "    for batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      out = self(batch.x, batch.edge_index)\n",
        "      loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "      total_loss += loss\n",
        "      acc += accuracy(out[batch.train_mask].argmax(dim=1),\n",
        "                      batch.y[batch.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Validation\n",
        "      val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n",
        "      val_acc += accuracy(out[batch.val_mask].argmax(dim=1),\n",
        "                          batch.y[batch.val_mask])\n",
        "```"
      ],
      "metadata": {
        "id": "y0k8e7klZOL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we have the full implementation of GraphSage"
      ],
      "metadata": {
        "id": "Z7MgS3dlZrMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGE(torch.nn.Module):\n",
        "  \"\"\"GraphSAGE\"\"\"\n",
        "  def __init__(self, dim_in, dim_h, dim_out, data):\n",
        "    super().__init__()\n",
        "    self.sage1 = SAGEConv(dim_in, dim_h)\n",
        "    self.sage2 = SAGEConv(dim_h, dim_out)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                      lr=0.01,\n",
        "                                      weight_decay=5e-4)\n",
        "    self.train_loader = NeighborLoader(\n",
        "      data,\n",
        "      num_neighbors=[5, 10],\n",
        "      batch_size=16,\n",
        "      input_nodes=data.train_mask,\n",
        "    )\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = self.sage1(x, edge_index).relu()\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.sage2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1)\n",
        "\n",
        "  def fit(self, data, epochs):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = self.optimizer\n",
        "\n",
        "    self.train()\n",
        "    for epoch in range(epochs+1):\n",
        "      total_loss = 0\n",
        "      acc = 0\n",
        "      val_loss = 0\n",
        "      val_acc = 0\n",
        "\n",
        "      # Train on batches\n",
        "      for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = self(batch.x, batch.edge_index)\n",
        "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "        total_loss += loss\n",
        "        acc += accuracy(out[batch.train_mask].argmax(dim=1),\n",
        "                        batch.y[batch.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n",
        "        val_acc += accuracy(out[batch.val_mask].argmax(dim=1),\n",
        "                            batch.y[batch.val_mask])\n",
        "\n",
        "      # Print metrics every 10 epochs\n",
        "      if(epoch % 10 == 0):\n",
        "          print(f'Epoch {epoch:>3} | Train Loss: {total_loss/len(train_loader):.3f} '\n",
        "                f'| Train Acc: {acc/len(train_loader)*100:>6.2f}% | Val Loss: '\n",
        "                f'{val_loss/len(train_loader):.2f} | Val Acc: '\n",
        "                f'{val_acc/len(train_loader)*100:.2f}%')"
      ],
      "metadata": {
        "id": "Bn_R-jifZoaq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing of model"
      ],
      "metadata": {
        "id": "jHCrbFSJZ5id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create GraphSAGE\n",
        "graphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes, data)\n",
        "print(graphsage)\n",
        "\n",
        "# Train\n",
        "graphsage.fit(data, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw9RzRjqaBbi",
        "outputId": "ae651794-9762-49b4-c222-dbf376dd83ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphSAGE(\n",
            "  (sage1): SAGEConv(500, 64, aggr=mean)\n",
            "  (sage2): SAGEConv(64, 3, aggr=mean)\n",
            ")\n",
            "Epoch   0 | Train Loss: 1.142 | Train Acc:  30.24% | Val Loss: 1.11 | Val Acc: 37.50%\n",
            "Epoch  10 | Train Loss: 0.078 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 64.79%\n",
            "Epoch  20 | Train Loss: 0.024 | Train Acc: 100.00% | Val Loss: 0.53 | Val Acc: 79.44%\n",
            "Epoch  30 | Train Loss: 0.023 | Train Acc: 100.00% | Val Loss: 0.55 | Val Acc: 76.01%\n",
            "Epoch  40 | Train Loss: 0.019 | Train Acc: 100.00% | Val Loss: 0.70 | Val Acc: 66.79%\n",
            "Epoch  50 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 72.29%\n",
            "Epoch  60 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 77.05%\n",
            "Epoch  70 | Train Loss: 0.026 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.92%\n",
            "Epoch  80 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 66.79%\n",
            "Epoch  90 | Train Loss: 0.021 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 80.90%\n",
            "Epoch 100 | Train Loss: 0.016 | Train Acc: 100.00% | Val Loss: 0.47 | Val Acc: 80.63%\n",
            "Epoch 110 | Train Loss: 0.016 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 67.87%\n",
            "Epoch 120 | Train Loss: 0.017 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 74.52%\n",
            "Epoch 130 | Train Loss: 0.010 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.15%\n",
            "Epoch 140 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.88%\n",
            "Epoch 150 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 0.31 | Val Acc: 88.93%\n",
            "Epoch 160 | Train Loss: 0.014 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 83.93%\n",
            "Epoch 170 | Train Loss: 0.018 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 84.17%\n",
            "Epoch 180 | Train Loss: 0.014 | Train Acc: 100.00% | Val Loss: 0.54 | Val Acc: 67.50%\n",
            "Epoch 190 | Train Loss: 0.012 | Train Acc: 100.00% | Val Loss: 0.69 | Val Acc: 66.34%\n",
            "Epoch 200 | Train Loss: 0.016 | Train Acc: 100.00% | Val Loss: 0.43 | Val Acc: 79.38%\n",
            "CPU times: user 4.63 s, sys: 18.7 ms, total: 4.65 s\n",
            "Wall time: 4.67 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nGraphSAGE test accuracy: {test(graphsage, data)*100:.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kvn1nMve7Ge",
        "outputId": "51338ee3-327e-4af4-94d8-b503501fc52c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GraphSAGE test accuracy: 75.50%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training time of GraphSAGE took significantly less amount of time compated to GCN and GAT.\n",
        "\n",
        "Even though this has the lowest accuracy among the 3, one should also put importance on scalability. GraphSAGE is by far the most scalable model since it can already have good results just by sampling data."
      ],
      "metadata": {
        "id": "fns3WHM9fEIt"
      }
    }
  ]
}